2019-06-03 21:40:12,412   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-03 21:40:12,765   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-03 21:40:12,777   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-0b14ede4-0d35-4f05-8e68-1d11af5d64f6-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-03 21:40:12,838   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 21:40:12,838   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 21:40:12,840   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-03 21:40:12,849   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-0b14ede4-0d35-4f05-8e68-1d11af5d64f6-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 21:40:12,900   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 21:40:12,900   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 21:40:12,901   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-03 21:40:12,902   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-0b14ede4-0d35-4f05-8e68-1d11af5d64f6-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 21:40:12,908   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 21:40:12,909   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 21:40:12,912   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-03 21:40:13,106   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-0b14ede4-0d35-4f05-8e68-1d11af5d64f6] State transition from CREATED to RUNNING.
2019-06-03 21:40:13,106   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-0b14ede4-0d35-4f05-8e68-1d11af5d64f6] Started Kafka Stream process
2019-06-03 21:40:13,106   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-03 21:40:13,124   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-03 21:40:15,383   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-03 22:18:32,938   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-03 22:18:33,329   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-03 22:18:33,341   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-7dc01552-f3bd-4c3b-af30-87196a955c8f-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-03 22:18:33,403   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:18:33,404   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:18:33,409   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-03 22:18:33,439   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-7dc01552-f3bd-4c3b-af30-87196a955c8f-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:18:33,505   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:18:33,506   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:18:33,506   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-03 22:18:33,506   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-7dc01552-f3bd-4c3b-af30-87196a955c8f-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:18:33,520   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:18:33,521   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:18:33,534   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-03 22:18:33,748   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-7dc01552-f3bd-4c3b-af30-87196a955c8f] State transition from CREATED to RUNNING.
2019-06-03 22:18:33,749   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-7dc01552-f3bd-4c3b-af30-87196a955c8f] Started Kafka Stream process
2019-06-03 22:18:33,749   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-03 22:18:33,926   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-03 22:18:36,188   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-03 22:19:39,706   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-03 22:19:55,182   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-03 22:19:55,219   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-117759d2-4132-4a5c-86ae-bcf7fe46a6c8-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-03 22:19:55,354   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:19:55,355   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:19:55,358   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-03 22:19:55,380   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-117759d2-4132-4a5c-86ae-bcf7fe46a6c8-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:19:55,498   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:19:55,498   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:19:55,499   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-03 22:19:55,502   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-117759d2-4132-4a5c-86ae-bcf7fe46a6c8-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:19:55,521   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:19:55,522   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:19:55,525   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-03 22:20:01,383   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-117759d2-4132-4a5c-86ae-bcf7fe46a6c8] State transition from CREATED to RUNNING.
2019-06-03 22:20:01,383   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-117759d2-4132-4a5c-86ae-bcf7fe46a6c8] Started Kafka Stream process
2019-06-03 22:20:01,383   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-03 22:20:03,243   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-03 22:20:05,506   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-03 22:21:01,333   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-03 22:21:06,726   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-03 22:21:06,760   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-7e05cf6e-1511-4ba6-8f7b-f48bc1525a2a-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-03 22:21:06,901   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:21:06,902   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:21:06,904   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-03 22:21:06,926   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-7e05cf6e-1511-4ba6-8f7b-f48bc1525a2a-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:21:07,046   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:21:07,047   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:21:07,047   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-03 22:21:07,050   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-7e05cf6e-1511-4ba6-8f7b-f48bc1525a2a-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:21:07,069   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:21:07,069   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:21:07,073   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-03 22:21:08,300   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-7e05cf6e-1511-4ba6-8f7b-f48bc1525a2a] State transition from CREATED to RUNNING.
2019-06-03 22:21:08,301   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-7e05cf6e-1511-4ba6-8f7b-f48bc1525a2a] Started Kafka Stream process
2019-06-03 22:21:08,301   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-03 22:21:10,356   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-03 22:21:12,620   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-03 22:22:43,150   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-03 22:22:43,538   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-03 22:22:43,552   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-0c63e2a0-8550-49a0-9c7e-3ba3ad8e79a7-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-03 22:22:43,635   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:22:43,635   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:22:43,637   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-03 22:22:43,649   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-0c63e2a0-8550-49a0-9c7e-3ba3ad8e79a7-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:22:43,705   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:22:43,705   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:22:43,706   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-03 22:22:43,706   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-0c63e2a0-8550-49a0-9c7e-3ba3ad8e79a7-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:22:43,716   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:22:43,717   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:22:43,719   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-03 22:22:43,929   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-0c63e2a0-8550-49a0-9c7e-3ba3ad8e79a7] State transition from CREATED to RUNNING.
2019-06-03 22:22:43,929   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-0c63e2a0-8550-49a0-9c7e-3ba3ad8e79a7] Started Kafka Stream process
2019-06-03 22:22:43,929   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-03 22:22:43,950   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-03 22:23:15,917   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-03 22:23:22,589   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-03 22:23:22,623   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-104751c1-fe9c-4bc7-bdd4-b4a3527cd7b6-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-03 22:23:22,754   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:23:22,755   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:23:22,758   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-03 22:23:22,780   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-104751c1-fe9c-4bc7-bdd4-b4a3527cd7b6-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:23:22,908   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:23:22,908   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:23:22,909   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-03 22:23:22,912   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-104751c1-fe9c-4bc7-bdd4-b4a3527cd7b6-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-03 22:23:22,931   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-03 22:23:22,931   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-03 22:23:22,935   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-03 22:23:24,524   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-104751c1-fe9c-4bc7-bdd4-b4a3527cd7b6] State transition from CREATED to RUNNING.
2019-06-03 22:23:24,524   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-104751c1-fe9c-4bc7-bdd4-b4a3527cd7b6] Started Kafka Stream process
2019-06-03 22:23:24,524   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-03 22:23:27,595   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-03 22:23:29,859   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-04 21:37:03,850   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-04 21:37:04,261   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-04 21:37:04,281   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-79cbb428-15a8-461d-ad5b-41468ffde874-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-04 21:37:04,341   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:37:04,341   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:37:04,341   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-04 21:37:04,351   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-79cbb428-15a8-461d-ad5b-41468ffde874-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:37:04,401   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:37:04,401   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:37:04,401   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-04 21:37:04,401   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-79cbb428-15a8-461d-ad5b-41468ffde874-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:37:04,411   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:37:04,411   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:37:04,411   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-04 21:37:04,611   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-79cbb428-15a8-461d-ad5b-41468ffde874] State transition from CREATED to RUNNING.
2019-06-04 21:37:04,611   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-79cbb428-15a8-461d-ad5b-41468ffde874] Started Kafka Stream process
2019-06-04 21:37:04,621   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-04 21:37:04,651   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-04 21:37:06,931   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-04 21:40:42,233   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-04 21:40:42,595   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-04 21:40:42,618   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-910b3fc6-6f85-4a18-8d9c-15d58ee287b5-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-04 21:40:42,686   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:40:42,686   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:40:42,688   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-04 21:40:42,708   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-910b3fc6-6f85-4a18-8d9c-15d58ee287b5-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:40:42,766   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:40:42,766   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:40:42,766   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-04 21:40:42,766   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-910b3fc6-6f85-4a18-8d9c-15d58ee287b5-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:40:42,779   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:40:42,779   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:40:42,781   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-04 21:40:43,000   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-910b3fc6-6f85-4a18-8d9c-15d58ee287b5] State transition from CREATED to RUNNING.
2019-06-04 21:40:43,000   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-910b3fc6-6f85-4a18-8d9c-15d58ee287b5] Started Kafka Stream process
2019-06-04 21:40:43,000   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-04 21:40:43,028   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-04 21:40:45,286   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-04 21:42:49,348   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-04 21:42:49,643   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-04 21:42:49,667   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-293e8c03-13ee-4c3a-a233-4911027b0591-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-04 21:42:49,725   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:42:49,725   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:42:49,725   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-04 21:42:49,736   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-293e8c03-13ee-4c3a-a233-4911027b0591-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:42:49,789   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:42:49,789   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:42:49,791   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-04 21:42:49,791   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-293e8c03-13ee-4c3a-a233-4911027b0591-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:42:49,797   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:42:49,797   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:42:49,799   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-04 21:42:49,996   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-293e8c03-13ee-4c3a-a233-4911027b0591] State transition from CREATED to RUNNING.
2019-06-04 21:42:49,996   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-293e8c03-13ee-4c3a-a233-4911027b0591] Started Kafka Stream process
2019-06-04 21:42:49,996   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-04 21:42:50,020   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-04 21:42:52,293   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-04 21:48:12,736   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-04 21:48:13,086   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-04 21:48:13,096   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-74c9baf9-0529-41a5-9d8e-e15c0b9e37a3-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-04 21:48:13,174   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:48:13,174   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:48:13,174   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-04 21:48:13,190   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-74c9baf9-0529-41a5-9d8e-e15c0b9e37a3-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:48:13,240   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:48:13,240   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:48:13,240   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-04 21:48:13,240   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-74c9baf9-0529-41a5-9d8e-e15c0b9e37a3-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:48:13,248   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:48:13,248   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:48:13,252   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-04 21:48:13,448   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-74c9baf9-0529-41a5-9d8e-e15c0b9e37a3] State transition from CREATED to RUNNING.
2019-06-04 21:48:13,448   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-74c9baf9-0529-41a5-9d8e-e15c0b9e37a3] Started Kafka Stream process
2019-06-04 21:48:13,448   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-04 21:48:13,471   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-04 21:48:15,731   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-04 21:48:52,958   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [192.168.43.31:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 192.168.43.31:2181

2019-06-04 21:49:22,941   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-04 21:49:22,971   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [192.168.43.31:9092]
	buffer.memory = 33554432
	client.id = logProcessor-92f9a80c-b316-495f-b70b-47c49eaeff6e-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-04 21:49:23,101   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:49:23,101   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:49:23,101   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-04 21:49:23,111   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-92f9a80c-b316-495f-b70b-47c49eaeff6e-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:49:23,221   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:49:23,221   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:49:23,221   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-04 21:49:23,221   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.43.31:9092]
	check.crcs = true
	client.id = logProcessor-92f9a80c-b316-495f-b70b-47c49eaeff6e-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-04 21:49:23,241   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-04 21:49:23,241   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-04 21:49:23,241   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-04 21:49:26,312   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-92f9a80c-b316-495f-b70b-47c49eaeff6e] State transition from CREATED to RUNNING.
2019-06-04 21:49:26,312   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-92f9a80c-b316-495f-b70b-47c49eaeff6e] Started Kafka Stream process
2019-06-04 21:49:26,312   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-04 21:49:27,950   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq5:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-04 21:49:30,215   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq5:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-06 21:10:16,171   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [47.101.131.128:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 47.101.131.128:2181

2019-06-06 21:11:43,908   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-06 21:11:43,947   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [47.101.131.128:9092]
	buffer.memory = 33554432
	client.id = logProcessor-c676cbc6-a844-4da0-b445-93d621161082-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-06 21:11:44,086   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:11:44,087   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:11:44,089   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-06 21:11:44,110   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-c676cbc6-a844-4da0-b445-93d621161082-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:11:44,234   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:11:44,234   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:11:44,235   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-06 21:11:44,238   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-c676cbc6-a844-4da0-b445-93d621161082-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:11:44,257   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:11:44,257   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:11:44,263   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-06 21:11:49,757   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-c676cbc6-a844-4da0-b445-93d621161082] State transition from CREATED to RUNNING.
2019-06-06 21:11:49,758   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-c676cbc6-a844-4da0-b445-93d621161082] Started Kafka Stream process
2019-06-06 21:11:49,758   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-06 21:12:35,316   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [47.101.131.128:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 47.101.131.128:2181

2019-06-06 21:12:35,747   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-06 21:12:35,762   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [47.101.131.128:9092]
	buffer.memory = 33554432
	client.id = logProcessor-240856c6-b9ba-4a40-b4e7-9dfb5dd326c5-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-06 21:12:35,850   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:12:35,850   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:12:35,852   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-06 21:12:35,862   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-240856c6-b9ba-4a40-b4e7-9dfb5dd326c5-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:12:35,921   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:12:35,921   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:12:35,921   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-06 21:12:35,922   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-240856c6-b9ba-4a40-b4e7-9dfb5dd326c5-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:12:35,929   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:12:35,930   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:12:35,932   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-06 21:12:38,066   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-240856c6-b9ba-4a40-b4e7-9dfb5dd326c5] State transition from CREATED to RUNNING.
2019-06-06 21:12:38,067   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-240856c6-b9ba-4a40-b4e7-9dfb5dd326c5] Started Kafka Stream process
2019-06-06 21:12:38,067   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-06 21:13:17,594   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [47.101.131.128:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 47.101.131.128:2181

2019-06-06 21:13:17,978   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-06 21:13:17,999   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [47.101.131.128:9092]
	buffer.memory = 33554432
	client.id = logProcessor-f94bb493-162b-4a05-80c3-db6f8b48b0a7-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-06 21:13:18,148   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:13:18,148   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:13:18,150   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-06 21:13:18,165   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-f94bb493-162b-4a05-80c3-db6f8b48b0a7-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:13:18,258   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:13:18,258   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:13:18,258   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-06 21:13:18,259   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-f94bb493-162b-4a05-80c3-db6f8b48b0a7-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:13:18,267   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:13:18,267   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:13:18,270   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-06 21:13:20,494   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-f94bb493-162b-4a05-80c3-db6f8b48b0a7] State transition from CREATED to RUNNING.
2019-06-06 21:13:20,495   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-f94bb493-162b-4a05-80c3-db6f8b48b0a7] Started Kafka Stream process
2019-06-06 21:13:20,495   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-06 21:13:22,433   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-06 21:13:24,690   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-06 21:15:09,116   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [47.101.131.128:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 47.101.131.128:2181

2019-06-06 21:15:09,537   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-06 21:15:09,555   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [47.101.131.128:9092]
	buffer.memory = 33554432
	client.id = logProcessor-370eff71-5039-4f51-8547-be4fc8fb1fa7-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-06 21:15:09,633   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:15:09,633   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:15:09,635   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-06 21:15:09,647   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-370eff71-5039-4f51-8547-be4fc8fb1fa7-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:15:09,715   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:15:09,716   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:15:09,716   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-06 21:15:09,717   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-370eff71-5039-4f51-8547-be4fc8fb1fa7-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:15:09,724   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:15:09,724   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:15:09,727   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-06 21:15:12,045   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-370eff71-5039-4f51-8547-be4fc8fb1fa7] State transition from CREATED to RUNNING.
2019-06-06 21:15:12,045   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-370eff71-5039-4f51-8547-be4fc8fb1fa7] Started Kafka Stream process
2019-06-06 21:15:12,045   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-06 21:15:13,923   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-06 21:15:16,180   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-06 21:16:33,530   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [47.101.131.128:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 47.101.131.128:2181

2019-06-06 21:16:38,543   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-06 21:16:38,559   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [47.101.131.128:9092]
	buffer.memory = 33554432
	client.id = logProcessor-2f5dbe64-b805-475a-a304-dcf6dee658c2-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-06 21:16:38,630   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:16:38,631   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:16:38,632   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-06 21:16:38,643   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-2f5dbe64-b805-475a-a304-dcf6dee658c2-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:16:38,701   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:16:38,701   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:16:38,701   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-06 21:16:38,702   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-2f5dbe64-b805-475a-a304-dcf6dee658c2-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:16:38,708   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:16:38,708   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:16:38,711   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-06 21:16:40,391   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-2f5dbe64-b805-475a-a304-dcf6dee658c2] State transition from CREATED to RUNNING.
2019-06-06 21:16:40,392   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-2f5dbe64-b805-475a-a304-dcf6dee658c2] Started Kafka Stream process
2019-06-06 21:16:40,392   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-06 21:16:41,881   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-06 21:16:44,139   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq:9092 (id: 2147483647 rack: null) dead for group logProcessor
2019-06-06 21:17:01,782   INFO --- [main]  org.apache.kafka.streams.StreamsConfig(line:180) : StreamsConfig values: 
	application.id = logProcessor
	application.server = 
	bootstrap.servers = [47.101.131.128:9092]
	buffered.records.per.partition = 1000
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 30000
	connections.max.idle.ms = 540000
	key.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	replication.factor = 1
	request.timeout.ms = 40000
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	windowstore.changelog.additional.retention.ms = 86400000
	zookeeper.connect = 47.101.131.128:2181

2019-06-06 21:17:05,668   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:304) : stream-thread [StreamThread-1] Creating producer client
2019-06-06 21:17:05,682   INFO --- [main]  org.apache.kafka.clients.producer.ProducerConfig(line:180) : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [47.101.131.128:9092]
	buffer.memory = 33554432
	client.id = logProcessor-ceb939e3-a303-445d-a8d4-da26a317d3dc-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2019-06-06 21:17:05,744   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:17:05,745   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:17:05,746   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:306) : stream-thread [StreamThread-1] Creating consumer client
2019-06-06 21:17:05,754   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-ceb939e3-a303-445d-a8d4-da26a317d3dc-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = logProcessor
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:17:05,799   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:17:05,799   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:17:05,799   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:317) : stream-thread [StreamThread-1] Creating restore consumer client
2019-06-06 21:17:05,800   INFO --- [main]  org.apache.kafka.clients.consumer.ConsumerConfig(line:180) : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [47.101.131.128:9092]
	check.crcs = true
	client.id = logProcessor-ceb939e3-a303-445d-a8d4-da26a317d3dc-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 2147483647
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2019-06-06 21:17:05,806   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:83) : Kafka version : 0.10.2.1
2019-06-06 21:17:05,806   INFO --- [main]  org.apache.kafka.common.utils.AppInfoParser(line:84) : Kafka commitId : e89bffd6b2eff799
2019-06-06 21:17:05,809   INFO --- [main]  org.apache.kafka.streams.processor.internals.StreamThread(line:163) : stream-thread [StreamThread-1] State transition from NOT_RUNNING to RUNNING.
2019-06-06 21:17:14,498   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:224) : stream-client [logProcessor-ceb939e3-a303-445d-a8d4-da26a317d3dc] State transition from CREATED to RUNNING.
2019-06-06 21:17:14,499   INFO --- [main]  org.apache.kafka.streams.KafkaStreams(line:436) : stream-client [logProcessor-ceb939e3-a303-445d-a8d4-da26a317d3dc] Started Kafka Stream process
2019-06-06 21:17:14,499   INFO --- [StreamThread-1]  org.apache.kafka.streams.processor.internals.StreamThread(line:358) : stream-thread [StreamThread-1] Starting
2019-06-06 21:17:16,155   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:586) : Discovered coordinator ywq:9092 (id: 2147483647 rack: null) for group logProcessor.
2019-06-06 21:17:18,413   INFO --- [StreamThread-1]  org.apache.kafka.clients.consumer.internals.AbstractCoordinator(line:631) : Marking the coordinator ywq:9092 (id: 2147483647 rack: null) dead for group logProcessor
